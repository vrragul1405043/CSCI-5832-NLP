# -*- coding: utf-8 -*-
"""RNN_Approach.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d35i5RT4D-5SPh5szAxKZDvFpUiqj_DJ
"""

!pip install tensorflow

import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import TextVectorization
from tensorflow.keras.layers import Embedding

print("Tensorflow Version:", tf.__version__)

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

trainFilePath = '/content/drive/MyDrive/NLP/S21-gene-train.txt'
glove50Path = '/content/drive/MyDrive/NLP/glove.6B.50d.txt'
modelPath = '/content/drive/MyDrive/NLP/Model/RNN/'
testFilePath = '/content/drive/MyDrive/NLP/F21-gene-test.txt'

def getRawData(path):
  with open(path) as f:
    rawData = f.readlines()
  return rawData

def processInput(rawData):
  tags = []
  sentence = []
  sentences = []
  labels = []
  for idx,line in enumerate(rawData):
    strippedLine = line.strip()
    if(len(strippedLine)>0):
      idx,word,tag = strippedLine.split("\t")
      sentence.append(word)
      tags.append(tag)
    else:
      # processedData.append([sentence,tags])
      sentences.append(sentence)
      labels.append(tags)
      sentence = []
      tags = []
   
  return sentences,labels

rawData = getRawData(trainFilePath)

sentences, labels = processInput(rawData)

print(len(sentences))

xTrain, xTest, yTrain, yTest = train_test_split(sentences,labels, test_size=0.15)

print(len(xTrain), len(xTest), len(yTrain), len(yTest))

"""## Storing word Frequencies to replace the words with less than threshold count as UNK"""

trainWordsList = list([tag for sentence in xTrain for tag in sentence])

trainWordsSet = set(trainWordsList)

np.random.seed(15)
toBeUNK = list(np.random.choice(list(trainWordsSet), 30, replace=False)) #replace UNK words add it manually

print(len(toBeUNK))

for word in toBeUNK:
  print("Word", word, "Freq", trainWordsList.count(word))

xTrainUpdated = []
toBeUNKCopy = toBeUNK

for sentence in xTrain:
  modified = []
  for word in sentence:
    if word in toBeUNKCopy:
      modified.append('UNK')
      toBeUNKCopy.pop(toBeUNKCopy.index(word)) 
    else:
      modified.append(word)
  xTrainUpdated.append(modified)

print(len(xTrainUpdated))

print("UNK Count", len([word for sentence in xTrainUpdated for word in sentence if word is 'UNK']))

"""## Prepare the data"""

# Helper method for Tag to Index
def getTokenIndexDict(vocabList):
    uniqueVocabulary = list(set([tag for sentence in vocabList for tag in sentence]))
    tokenToIndex = {token:idx for idx, token in enumerate(uniqueVocabulary)}
    indexToToken = {idx:token for idx, token in enumerate(uniqueVocabulary)}
    return tokenToIndex, indexToToken

tagToIndex, indexToTag = getTokenIndexDict(yTrain)

print("Tag to index", tagToIndex)
print("Index to Tag", indexToTag)

trainTagEncodings = [[tagToIndex[tag] for tag in sentence] for sentence in yTrain]

print("Train tag Encodings", len(trainTagEncodings))

sentenceToIndex, indexToSentence = getTokenIndexDict(xTrainUpdated)

sentenceToIndex['PAD'] = len(sentenceToIndex)
indexToSentence[len(indexToSentence)]='PAD'

trainSentenceEncodings = [[sentenceToIndex[tag] for tag in sentence] for sentence in xTrainUpdated]

print("Train Sentence Encodings", len(trainSentenceEncodings))

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

def getPaddedData(sentenceEncodings, labelEncodings, numTags, maxLen):
  padTokens = pad_sequences(sentenceEncodings, maxlen=maxLen, dtype='int32', padding='post', value=sentenceToIndex['PAD'])
  padTags = pad_sequences(labelEncodings, maxlen=maxLen, dtype='int32', padding='post', value= tagToIndex["O"])
  padTags = [to_categorical(i, num_classes=numTags) for i in padTags]
  return padTokens,np.array(padTags)

padTokens, padTags = getPaddedData(trainSentenceEncodings,trainTagEncodings,3,300)

padTokens.shape

padTags.shape

# any([2 in sentence for sentence in padTags])

"""## Build the Model"""

from tensorflow.keras import Sequential, Model, Input
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional
from tensorflow.keras.utils import plot_model

def getModel():
    model = Sequential()

    # Add Embedding layer
    model.add(Embedding(input_dim=len(trainWordsSet)+2, output_dim=512))
    
    # Add bidirectional LSTM
    model.add(Bidirectional(LSTM(units=200, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))
    model.add(Dropout(0.2))

    # Add LSTM
    model.add(LSTM(units=100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))
    model.add(Dropout(0.1))

    #Optimiser 
    model.add(Dense(3, activation="softmax"))

    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['AUC'])
    model.summary()
    
    return model

model = getModel()

model.fit(padTokens, padTags, batch_size=128, verbose=1, epochs=5)

model.save(modelPath)

"""##Preparing the test data"""

trainSetUpdatedWords = set([word for sentence in xTrainUpdated for word in sentence])

xTestUpdated = [['UNK' if word not in trainSetUpdatedWords else word for word in sentence]for sentence in xTest]

print("Unk Count", len([word for sentence in xTestUpdated for word in sentence if word=='UNK']))

print("Total Words", len([word for sentence in xTestUpdated for word in sentence]))

testSentenceEncodings = [[sentenceToIndex[tag] for tag in sentence] for sentence in xTestUpdated]

testTagEncodings = [[tagToIndex[tag] for tag in sentence] for sentence in yTest]

padTestTokens, padTestTags = getPaddedData(testSentenceEncodings,testTagEncodings,3,300)

predictedTagTokens = model.predict(padTestTokens)

yPredIndices = np.argmax(predictedTagTokens,axis=2)

yPredIndices.shape

yPredicted = np.array([[indexToTag[index] for index in sentence] for sentence in yPredIndices])

np.unique(yPredicted)

print(yPredicted.shape)

ActualValuesPath = '/content/drive/MyDrive/NLP/golden.txt'
PredictedValuesPath = '/content/drive/MyDrive/NLP/predicted.txt'
TestResultsPath = '/content/drive/MyDrive/NLP/test_results.txt'

with open(ActualValuesPath, 'w') as actual, open(PredictedValuesPath, 'w') as predict:
  for idx, pred in enumerate(yPredicted):
    # actualLength = len(yTest[idx])
    for lineNumber, word in enumerate(xTest[idx]):
      actual.write(f"{lineNumber+1}\t{word}\t{yTest[idx][lineNumber]}")
      actual.write("\n")
      predict.write(f"{lineNumber+1}\t{word}\t{pred[lineNumber]}")
      predict.write("\n")
    actual.write("\n")
    predict.write("\n")

"""## Predicting the test data"""

def processTestData(rawData):
  sentence = []
  sentences = []
  for idx,line in enumerate(rawData):
    strippedLine = line.strip()
    if(len(strippedLine)>0):
      idx,word = strippedLine.split("\t")
      sentence.append(word)
      
    else:
      # processedData.append([sentence,tags])
      sentences.append(sentence)
      sentence = []
  return sentences

testRawData = getRawData(testFilePath)

testSentences = processTestData(testRawData)

print(len(testSentences))

xTestDataUpdated = [['UNK' if word not in trainSetUpdatedWords else word for word in sentence]for sentence in testSentences]

len(xTestDataUpdated)

print("UNK word Count", len([word for sentence in xTestDataUpdated for word in sentence if word=='UNK']))

testDataSentenceEncodings = [[sentenceToIndex[tag] for tag in sentence] for sentence in xTestDataUpdated]

paddedTestData = pad_sequences(testDataSentenceEncodings, maxlen=300, dtype='int32', padding='post', value=sentenceToIndex['PAD'])

paddedTestData.shape

predictedTestTagTokens = model.predict(paddedTestData)

yTestPredIndices = np.argmax(predictedTestTagTokens,axis=2)

yTestPredicted = np.array([[indexToTag[index] for index in sentence] for sentence in yTestPredIndices])

yTestPredicted.shape

np.unique(yTestPredicted)

with open(TestResultsPath, 'w') as testResults:
  for idx, pred in enumerate(yTestPredicted):
    # actualLength = len(yTest[idx])
    for lineNumber, word in enumerate(testSentences[idx]):
      testResults.write(f"{lineNumber+1}\t{word}\t{yTestPredicted[idx][lineNumber]}")
      testResults.write("\n")
    testResults.write("\n")